"""
Article Writer - Layer 2

ä½¿ç”¨ google/gemini-3-flash-preview via OpenRouter ç”Ÿæˆæ–‡ç« ã€‚
è¯»å– rerank_cache å’Œ web_search_cacheï¼Œæ ¹æ®é£æ ¼æ¨¡æ¿ç”Ÿæˆæ–‡ç« ã€‚

æ”¯æŒ 5 ç§é£æ ¼:
- 36kr: ä¸“ä¸šã€æ•°æ®é©±åŠ¨ã€è¡Œä¸šåˆ†æ
- huxiu: çŠ€åˆ©ã€æœ‰æ€åº¦ã€æ·±åº¦è¯„è®º
- xiaohongshu: è½»æ¾ã€å£è¯­åŒ–ã€åˆ†ç‚¹åˆ—ä¸¾
- linkedin: èŒåœºè§†è§’ã€å¼ºè°ƒæœºä¼š
- zhihu: çŸ¥è¯†åˆ†äº«ã€é€»è¾‘æ¸…æ™°
"""

import json
import os
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

from openai import OpenAI

from .models import Article, ArticleStyle, RerankResult, WebSearchResult
from .styles import ARTICLE_STYLES, get_style


class ArticleWriter:
    """æ–‡ç« ç”Ÿæˆå™¨
    
    åŸºäºç²¾æ’ç»“æœå’Œ Web æœç´¢ç¼“å­˜ï¼ŒæŒ‰æŒ‡å®šé£æ ¼ç”Ÿæˆæ–‡ç« ã€‚
    
    Example:
        writer = ArticleWriter()
        article = writer.write_article(rerank_result, style="36kr", web_search_cache={})
    """
    
    DEFAULT_MODEL = "google/gemini-3-flash-preview"
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
    ):
        """åˆå§‹åŒ–æ–‡ç« ç”Ÿæˆå™¨
        
        Args:
            api_key: OpenRouter API key
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ gemini-3-flash-previewï¼‰
        """
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY is required")
        
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=self.api_key,
        )
        self.model = model or self.DEFAULT_MODEL
    
    def _build_article_prompt(
        self,
        rerank_result: RerankResult,
        style: ArticleStyle,
        web_search_cache: Dict[str, WebSearchResult],
        query_company_details: str = "",
    ) -> str:
        """æ„å»ºæ–‡ç« ç”Ÿæˆ prompt
        
        Args:
            rerank_result: ç²¾æ’ç»“æœ
            style: æ–‡ç« é£æ ¼é…ç½®
            web_search_cache: Web æœç´¢ç¼“å­˜
            query_company_details: æŸ¥è¯¢å…¬å¸çš„è¯¦ç»†ä¿¡æ¯
        """
        # æ„å»ºæ¨èå…¬å¸ä¿¡æ¯
        company_sections = []
        for i, sc in enumerate(rerank_result.selected_companies, 1):
            section = f"""
### æ¨èå…¬å¸ {i}: {sc.company_name}
- å…¬å¸ID: {sc.company_id}
- åœ°ç‚¹: {sc.location}
- å…¬å¸ä»‹ç»: {sc.company_details[:800] if sc.company_details else 'æ— è¯¦ç»†ä¿¡æ¯'}
- é€‰æ‹©ç†ç”±: {sc.selection_reason}
"""
            # æ·»åŠ  web search ç»“æœ
            if sc.company_id in web_search_cache:
                ws = web_search_cache[sc.company_id]
                if ws.is_valid:
                    section += f"""
**ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯:**
{ws.search_summary[:1500]}

å¼•ç”¨æ¥æº: {', '.join(ws.citations[:5]) if ws.citations else 'æ— '}
"""
            company_sections.append(section)
        
        companies_text = "\n".join(company_sections)
        
        # æŸ¥è¯¢å…¬å¸çš„ web search ä¿¡æ¯
        query_web_info = ""
        if rerank_result.query_company_id in web_search_cache:
            ws = web_search_cache[rerank_result.query_company_id]
            if ws.is_valid:
                query_web_info = f"""
**ç½‘ç»œæœç´¢è¡¥å……ä¿¡æ¯:**
{ws.search_summary[:1500]}
"""
        
        # Emoji æŒ‡ç¤º
        emoji_instruction = ""
        if style.use_emoji:
            emoji_instruction = """
**é‡è¦**: è¯·åœ¨æ ‡é¢˜å’Œæ­£æ–‡ä¸­é€‚å½“ä½¿ç”¨ emoji å¢åŠ è¶£å‘³æ€§å’Œå¯è¯»æ€§ã€‚
å¸¸ç”¨ emoji: ğŸ”¥ âœ¨ ğŸ’¡ ğŸ“Š ğŸš€ ğŸ’° ğŸ¯ ğŸ‘€ ğŸ“ˆ â­ ğŸ† ğŸ’ª
"""
        else:
            emoji_instruction = """
**é‡è¦**: è¯·å‹¿ä½¿ç”¨ä»»ä½• emojiï¼Œä¿æŒä¸“ä¸šä¸¥è‚ƒçš„é£æ ¼ã€‚
"""
        
        return f"""ä½ æ˜¯{style.name_zh}çš„èµ„æ·±æ’°ç¨¿äººã€‚ç°åœ¨æ‰‹é‡Œæœ‰ä¸€ç»„å…¬å¸ç´ æï¼Œå†™ä¸€ç¯‡æ·±åº¦ç¨¿ã€‚

## ä½ çš„å†™ä½œäººè®¾
{style.tone}
{emoji_instruction}

## å†™æ³•æŒ‡å¯¼
{style.structure}

## å¼€å¤´å¯ä»¥å‚è€ƒè¿™ä¸ªæ„Ÿè§‰ï¼ˆä¸è¦ç…§æŠ„ï¼‰
"{style.example_intro}"

---

## ç´ æ

**ä½ è¦å†™çš„ä¸»çº¿/è§’åº¦**: {rerank_result.narrative_angle}

**ä¸»è§’å…¬å¸ï¼ˆé‡ç‚¹å†™ï¼‰**:
{rerank_result.query_company_name}
{query_company_details[:800] if query_company_details else 'æš‚æ— è¯¦æƒ…'}
{query_web_info}

**ç›¸å…³å…¬å¸ï¼ˆé…è§’ï¼Œçµæ´»ä½¿ç”¨ï¼‰**:
{companies_text}

---

## æ³¨æ„äº‹é¡¹
- å­—æ•° {style.word_count_min}-{style.word_count_max} å­—
- ä¸è¦å†™æˆç™¾ç§‘ä»‹ç»ï¼Œè¦åƒè®°è€…å†™æŠ¥é“ä¸€æ ·æœ‰èŠ‚å¥æœ‰è§‚ç‚¹
- é…è§’å…¬å¸ä¸éœ€è¦æ¯å®¶éƒ½å†™ï¼ŒæŒ‘æœ‰å¯¹æ¯”ä»·å€¼æˆ–èƒ½æ¨è¿›å™äº‹çš„å†™
- å…¬å¸ä¹‹é—´çš„è¿‡æ¸¡è¦è‡ªç„¶ï¼Œåˆ«ç”¨"æ¥ä¸‹æ¥ä»‹ç»"è¿™ç§è¯
- æ ‡é¢˜è¦æŠ“äººï¼Œåˆ«å¤ªå­¦æœ¯è…”

## è¾“å‡º
JSONæ ¼å¼ï¼Œä¸‰ä¸ªå­—æ®µï¼š
```json
{{
  "title": "æ ‡é¢˜",
  "content": "æ­£æ–‡ï¼ˆmarkdownï¼‰",
  "key_takeaways": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
}}
```
"""
    
    def _parse_article_response(
        self,
        response_text: str,
        rerank_result: RerankResult,
        style_id: str,
    ) -> Article:
        """è§£ææ–‡ç« ç”Ÿæˆå“åº”"""
        # æå–æ‰€æœ‰å€™é€‰å…¬å¸ ID
        candidate_company_ids = [
            sc.company_id for sc in rerank_result.selected_companies
        ]
        
        try:
            # å°è¯•æå– JSON
            json_text = response_text
            if "```json" in response_text:
                json_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                json_text = response_text.split("```")[1].split("```")[0]
            
            data = json.loads(json_text.strip())
        except (json.JSONDecodeError, IndexError):
            # è§£æå¤±è´¥ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”æ–‡æœ¬
            return Article(
                query_company_id=rerank_result.query_company_id,
                query_company_name=rerank_result.query_company_name,
                rule_id=rerank_result.rule_id,
                style=style_id,
                title="ç”Ÿæˆå¤±è´¥",
                content=response_text,
                word_count=len(response_text),
                candidate_company_ids=candidate_company_ids,
                key_takeaways=[],
                citations=[],
            )
        
        content = data.get("content", "")
        
        return Article(
            query_company_id=rerank_result.query_company_id,
            query_company_name=rerank_result.query_company_name,
            rule_id=rerank_result.rule_id,
            style=style_id,
            title=data.get("title", "æ— æ ‡é¢˜"),
            content=content,
            word_count=len(content),
            candidate_company_ids=candidate_company_ids,
            key_takeaways=data.get("key_takeaways", []),
            citations=self._extract_citations(content),
        )
    
    def _extract_citations(self, text: str) -> List[str]:
        """ä» markdown æ–‡æœ¬ä¸­æå–å¼•ç”¨é“¾æ¥"""
        import re
        pattern = r'\[([^\]]+)\]\(([^)]+)\)'
        matches = re.findall(pattern, text)
        return list(set(url for _, url in matches))
    
    def write_article(
        self,
        rerank_result: RerankResult,
        style_id: str,
        web_search_cache: Dict[str, WebSearchResult],
        query_company_details: str = "",
    ) -> Article:
        """ç”Ÿæˆå•ç¯‡æ–‡ç« 
        
        Args:
            rerank_result: ç²¾æ’ç»“æœ
            style_id: é£æ ¼ID (36kr, huxiu, xiaohongshu, linkedin, zhihu)
            web_search_cache: Web æœç´¢ç¼“å­˜
            query_company_details: æŸ¥è¯¢å…¬å¸è¯¦æƒ…
            
        Returns:
            Article ç”Ÿæˆçš„æ–‡ç« 
        """
        style = get_style(style_id)
        if not style:
            raise ValueError(f"Unknown style: {style_id}")
        
        prompt = self._build_article_prompt(
            rerank_result=rerank_result,
            style=style,
            web_search_cache=web_search_cache,
            query_company_details=query_company_details,
        )
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7,  # ç¨é«˜æ¸©åº¦å¢åŠ åˆ›æ„
            )
            
            response_text = response.choices[0].message.content or "{}"
            
            return self._parse_article_response(
                response_text=response_text,
                rerank_result=rerank_result,
                style_id=style_id,
            )
            
        except Exception as e:
            return Article(
                query_company_id=rerank_result.query_company_id,
                query_company_name=rerank_result.query_company_name,
                rule_id=rerank_result.rule_id,
                style=style_id,
                title="ç”Ÿæˆå¤±è´¥",
                content=f"Error: {str(e)}",
                word_count=0,
                candidate_company_ids=[sc.company_id for sc in rerank_result.selected_companies],
                key_takeaways=[],
                citations=[],
            )
    
    def batch_write(
        self,
        rerank_results: List[RerankResult],
        style_ids: List[str],
        web_search_cache: Dict[str, WebSearchResult],
        company_details_map: Dict[str, str],
        delay_seconds: float = 0.5,
        skip_existing: bool = True,
        output_dir: Optional[Path] = None,
        show_progress: bool = True,
    ) -> List[Article]:
        """æ‰¹é‡ç”Ÿæˆæ–‡ç« 
        
        Args:
            rerank_results: ç²¾æ’ç»“æœåˆ—è¡¨
            style_ids: é£æ ¼IDåˆ—è¡¨
            web_search_cache: Web æœç´¢ç¼“å­˜
            company_details_map: {company_id: company_details}
            delay_seconds: è¯·æ±‚é—´éš”
            skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰æ–‡ç« 
            output_dir: è¾“å‡ºç›®å½•
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            Article åˆ—è¡¨
        """
        from tqdm import tqdm
        
        articles = []
        
        # å±•å¼€æ‰€æœ‰ (rerank_result, style) å¯¹
        tasks = []
        for rr in rerank_results:
            for style_id in style_ids:
                tasks.append({
                    "rerank_result": rr,
                    "style_id": style_id,
                })
        
        iterator = tasks
        if show_progress:
            iterator = tqdm(tasks, desc="Writing articles")
        
        for task in iterator:
            rr = task["rerank_result"]
            style_id = task["style_id"]
            
            # æ–‡ä»¶å: {query_company_id}_{rule_id}_{style}.json
            filename = f"{rr.query_company_id}_{rr.rule_id}_{style_id}.json"
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if skip_existing and output_dir:
                output_file = output_dir / filename
                if output_file.exists():
                    try:
                        with open(output_file, "r", encoding="utf-8") as f:
                            cached = json.load(f)
                        articles.append(Article(
                            query_company_id=cached["query_company_id"],
                            query_company_name=cached["query_company_name"],
                            rule_id=cached["rule_id"],
                            style=cached["style"],
                            title=cached["title"],
                            content=cached["content"],
                            word_count=cached.get("word_count", 0),
                            candidate_company_ids=cached.get("candidate_company_ids", []),
                            key_takeaways=cached.get("key_takeaways", []),
                            citations=cached.get("citations", []),
                            generated_at=cached.get("generated_at", ""),
                        ))
                        continue
                    except Exception:
                        pass
            
            # ç”Ÿæˆæ–‡ç« 
            article = self.write_article(
                rerank_result=rr,
                style_id=style_id,
                web_search_cache=web_search_cache,
                query_company_details=company_details_map.get(rr.query_company_id, ""),
            )
            articles.append(article)
            
            # ä¿å­˜æ–‡ç« 
            if output_dir:
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / filename
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump({
                        "query_company_id": article.query_company_id,
                        "query_company_name": article.query_company_name,
                        "rule_id": article.rule_id,
                        "style": article.style,
                        "title": article.title,
                        "content": article.content,
                        "word_count": article.word_count,
                        "candidate_company_ids": article.candidate_company_ids,
                        "key_takeaways": article.key_takeaways,
                        "citations": article.citations,
                        "generated_at": article.generated_at,
                    }, f, ensure_ascii=False, indent=2)
                
                # åŒæ—¶ä¿å­˜ markdown ç‰ˆæœ¬
                md_file = output_dir / filename.replace(".json", ".md")
                with open(md_file, "w", encoding="utf-8") as f:
                    f.write(f"# {article.title}\n\n")
                    f.write(f"> é£æ ¼: {article.style} | è§„åˆ™: {article.rule_id}\n")
                    f.write(f"> æ¨èå…¬å¸: {', '.join(article.candidate_company_ids)}\n\n")
                    f.write(article.content)
                    if article.key_takeaways:
                        f.write("\n\n---\n\n## æ ¸å¿ƒè¦ç‚¹\n\n")
                        for takeaway in article.key_takeaways:
                            f.write(f"- {takeaway}\n")
            
            # æ·»åŠ å»¶è¿Ÿ
            if delay_seconds > 0:
                time.sleep(delay_seconds)
        
        return articles


def load_articles(articles_dir: Path) -> Dict[str, Article]:
    """åŠ è½½å·²ç”Ÿæˆçš„æ–‡ç« 
    
    Args:
        articles_dir: æ–‡ç« ç›®å½•
        
    Returns:
        {"{company_id}_{rule_id}_{style}": Article}
    """
    articles = {}
    
    if not articles_dir.exists():
        return articles
    
    for json_file in articles_dir.glob("*.json"):
        if json_file.name == "run_metadata.json":
            continue
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            key = f"{data['query_company_id']}_{data['rule_id']}_{data['style']}"
            articles[key] = Article(
                query_company_id=data["query_company_id"],
                query_company_name=data["query_company_name"],
                rule_id=data["rule_id"],
                style=data["style"],
                title=data["title"],
                content=data["content"],
                word_count=data.get("word_count", 0),
                candidate_company_ids=data.get("candidate_company_ids", []),
                key_takeaways=data.get("key_takeaways", []),
                citations=data.get("citations", []),
                generated_at=data.get("generated_at", ""),
            )
        except Exception:
            continue
    
    return articles

